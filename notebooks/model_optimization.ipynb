{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdfd714b",
   "metadata": {},
   "source": [
    "# Model Optimization Notebook\n",
    "\n",
    "This Jupyter Notebook demonstrates the process of preparing image data, downloading the COCO dataset, preprocessing images, and converting a YOLO-NAS S ONNX model to various SNPE DLC formats for deployment. The workflow includes:\n",
    "\n",
    "- Importing necessary libraries for image processing and file management.\n",
    "- Downloading and extracting a subset of the COCO validation dataset.\n",
    "- Preprocessing images to the required input format for model inference.\n",
    "- Converting the YOLO-NAS S ONNX model to SNPE DLC format, including quantization and graph preparation for specific hardware targets.\n",
    "- Documenting each step for reproducibility and clarity.\n",
    "\n",
    "This notebook serves as a practical guide for deploying deep learning models on Qualcomm platforms using the SNPE toolkit.\n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. **Build and start the Docker Compose environment** as described in the project documentation.\n",
    "2. **Access this notebook** in your browser at:  \n",
    "    [http://127.0.0.1:8888/notebooks/model_optimization.ipynb](http://127.0.0.1:8888/notebooks/model_optimization.ipynb)\n",
    "3. **Run all cells** in order to optimiza the ONNX model to Qualcomm chipsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa434004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682e84b",
   "metadata": {},
   "source": [
    "## Data cleaning.\n",
    "\n",
    "The `preprocess` function resizes an input image to 320x320 pixels, normalizes its pixel values to the range [0, 1], and returns the processed image as a NumPy array of type float32, preparing it for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fafef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(original_image: np.ndarray, size: int = 320) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess the input image for model inference.\n",
    "\n",
    "    Args:\n",
    "        original_image (np.ndarray): The input image in BGR format.\n",
    "        size (int): The target size for resizing the image. Default is 320.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The preprocessed image in the format expected by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Resize the image to (size X size) pixels and normalize pixel values to\n",
    "    # [0, 1].\n",
    "    resized_image = cv.resize(original_image, (size, size))\n",
    "    return (resized_image / 255.).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a94d9c",
   "metadata": {},
   "source": [
    "### Getting the coco dataset\n",
    "The COCO (Common Objects in Context) dataset is a large-scale image dataset designed for object detection, segmentation, and captioning tasks. In this pipeline, we use a subset of the COCO validation images to test and optimize our deep learning model. The images are downloaded, preprocessed, and converted into a raw format suitable for model inference and quantization steps. This ensures that the model is evaluated and optimized using real-world, diverse data representative of common objects and scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0d862-08db-4b97-8304-d2649bdb49e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"val2017.zip\"):\n",
    "    !zsh -c 'wget http://images.cocodataset.org/zips/val2017.zip -q --show-progress'\n",
    "\n",
    "if not os.path.exists(\"val2017\"):\n",
    "    !zsh -c 'unzip val2017.zip'\n",
    "\n",
    "if not os.path.exists(\"val.zip\"):\n",
    "    !zsh -c 'wget https://huggingface.co/datasets/testdummyvt/hagRIDv2_512px_10GB/resolve/main/val.zip -q --show-progress'\n",
    "\n",
    "if not os.path.exists(\"val\"):\n",
    "    !zsh -c 'unzip val.zip'\n",
    "\n",
    "if not os.path.exists(\"raw\"):\n",
    "    !zsh -c 'mkdir raw'\n",
    "    !zsh -c 'mkdir raw/coco'\n",
    "    !zsh -c 'mkdir raw/hagRID'\n",
    "\n",
    "    random.seed(42)\n",
    "    for dataset, folder in zip([\"coco\", \"hagRID\"], [\"val2017\", \"val\"]):\n",
    "        filenames = glob.glob(f\"{folder}/**/*.jpg\", recursive=True)\n",
    "        random.shuffle(filenames)\n",
    "\n",
    "        for filename in filenames[:100]:\n",
    "            image = cv.imread(filename)\n",
    "\n",
    "            # Preprocess the image and save it in raw format.\n",
    "            normalized_image = preprocess(image)\n",
    "            normalized_image.tofile(f\"raw/{dataset}/{uuid.uuid4()}.raw\")\n",
    "\n",
    "!zsh -c 'rm -rf __MACOSX'\n",
    "!zsh -c 'find raw/coco -name \"*.raw\" > ./raw/coco/input.txt'\n",
    "!zsh -c 'find raw/hagRID -name \"*.raw\" > ./raw/hagRID/input.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b3cf9",
   "metadata": {},
   "source": [
    "## Model Optimization (YOLO-NAS S)\n",
    "This section covers the process of optimizing a deep learning model for deployment on Qualcomm® chipsets using the Qualcomm® Neural Processing SDK for AI (SNPE). The workflow includes converting a YOLO NAS ONNX model to the SNPE DLC format, quantizing the model for efficient inference, and preparing the model for specific hardware targets.\n",
    "\n",
    "### 1. Model Conversion\n",
    "\n",
    "The first step is to convert the ONNX model to the SNPE Deep Learning Container (DLC) format. This is achieved using the `snpe-onnx-to-dlc` tool, which translates the ONNX model into a format compatible with Qualcomm® hardware accelerators.\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-onnx-to-dlc -i /models/yolo_nas_s.onnx -o /models/yolo_nas_s_fp32.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1798bae-ec56-417c-b41b-7704ad8261ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-onnx-to-dlc -i /models/yolo_nas_s.onnx -o /models/yolo_nas_s_fp32.dlc'\n",
    "!zsh -c 'rm -rf -p output'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe981f",
   "metadata": {},
   "source": [
    "### 2. Model Inspection\n",
    "\n",
    "After conversion, the `snpe-dlc-info` tool is used to inspect the DLC file. This step ensures that the model has been correctly converted and provides information about the model's input and output tensors.\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-dlc-info -i /models/yolo_nas_s_fp32.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79b43f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-dlc-info -i /models/yolo_nas_s_fp32.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb9fc9",
   "metadata": {},
   "source": [
    "### 3. Model Quantization\n",
    "\n",
    "Quantization reduces the model size and increases inference speed by converting floating-point weights to 8-bit integers. The `snpe-dlc-quantize` tool uses a calibration dataset (prepared in the previous steps) to optimize the model for INT8 precision.\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-dlc-quantize --input_dlc /models/yolo_nas_s_fp32.dlc --input_list ./raw/coco/input.txt --output_dlc /models/yolo_nas_s_int8.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abad4e88-e265-46e8-9b5e-8c87d86ffa1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-dlc-quantize --input_dlc /models/yolo_nas_s_fp32.dlc --input_list ./raw/coco/input.txt --output_dlc /models/yolo_nas_s_int8.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf28a16",
   "metadata": {},
   "source": [
    "### 4. Post-Quantization Inspection\n",
    "\n",
    "After quantization, the model is inspected again to verify the changes and ensure the quantized model is ready for deployment.\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-dlc-info -i /models/yolo_nas_s_int8.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea2f01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-dlc-info -i /models/yolo_nas_s_int8.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c66503",
   "metadata": {},
   "source": [
    "### 5. Hardware-Specific Graph Preparation\n",
    "\n",
    "To further optimize the model for a specific Qualcomm® SoC (e.g., SM7325), the `snpe-dlc-graph-prepare` tool is used. This step configures the model's output tensors and prepares it for execution on the target hardware's HTP (Hexagon Tensor Processor).\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-dlc-graph-prepare --input_dlc /models/yolo_nas_s_int8.dlc --set_output_tensors=output_bboxes,output_classes --htp_socs=sm7325 --output_dlc=/models/yolo_nas_s_int8_htp_sm7325.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aeeea1-9429-447a-b4a9-894c45e6bab6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-dlc-graph-prepare --input_dlc /models/yolo_nas_s_int8.dlc --set_output_tensors=output_bboxes,output_classes --htp_socs=sm7325 --output_dlc=/models/yolo_nas_s_int8_htp_sm7325.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5cec5",
   "metadata": {},
   "source": [
    "### 6. Final Model Inspection\n",
    "\n",
    "A final inspection confirms that the model is correctly prepared for the target hardware and ready for deployment.\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-dlc-info -i /models/yolo_nas_s_int8_htp_sm7325.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a22362",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-dlc-info -i /models/yolo_nas_s_int8_htp_sm7325.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998a187d",
   "metadata": {},
   "source": [
    "## Model Optimization (YOLO-hagRID)\n",
    "This section details the optimization workflow for the YOLO-hagRID model, following the same steps as with YOLO-NAS S. The process includes converting the ONNX model to SNPE DLC format, quantizing for efficient inference, and preparing the model for deployment on Qualcomm® hardware. Each step ensures the model is tailored for high performance and low latency on edge devices.\n",
    "\n",
    "### 1. Model Conversion\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-onnx-to-dlc -i /models/yolo_hagRID.onnx -o /models/yolo_hagRID_fp32.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42439076",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-onnx-to-dlc -i /models/yolo_hagRID.onnx -o /models/yolo_hagRID_fp32.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ea277",
   "metadata": {},
   "source": [
    "### 2. Model Inspection\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-dlc-info -i /models/yolo_hagRID_fp32.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868399cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-dlc-info -i /models/yolo_hagRID_fp32.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e72460",
   "metadata": {},
   "source": [
    "### 3. Model Quantization\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-dlc-quantize --input_dlc /models/yolo_hagRID_fp32.dlc --input_list ./raw/hagRID/input.txt --output_dlc /models/yolo_hagRID_int8.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694e7215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-dlc-quantize --input_dlc /models/yolo_hagRID_fp32.dlc --input_list ./raw/hagRID/input.txt --output_dlc /models/yolo_hagRID_int8.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd56b8",
   "metadata": {},
   "source": [
    "### 4. Post-Quantization Inspection\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-dlc-info -i /models/yolo_hagRID_int8.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869f377",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-dlc-info -i /models/yolo_hagRID_int8.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d726b85f",
   "metadata": {},
   "source": [
    "### 5. Hardware-Specific Graph Preparation\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-dlc-graph-prepare --input_dlc /models/yolo_hagRID_int8.dlc --set_output_tensors=output_bboxes,output_classes --htp_socs=sm7325 --output_dlc=/models/yolo_hagRID_int8_htp_sm7325.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f65f19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-dlc-graph-prepare --input_dlc /models/yolo_hagRID_int8.dlc --set_output_tensors=output_bboxes,output_classes --htp_socs=sm7325 --output_dlc=/models/yolo_hagRID_int8_htp_sm7325.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79559aea",
   "metadata": {},
   "source": [
    "### 6. Final Model Inspection\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-dlc-info -i /models/yolo_hagRID_int8_htp_sm7325.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a295db21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-dlc-info -i /models/yolo_hagRID_int8_htp_sm7325.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3044d17",
   "metadata": {},
   "source": [
    "## Model Optimization (TensorFlow to TFLite)\n",
    "This section demonstrates how to convert TensorFlow SavedModels to TensorFlow Lite (TFLite) format for deployment on mobile and edge devices. The process iterates over two models (`yolo_nas_s` and `yolo_hagRID`), loading each SavedModel and converting it to a TFLite model using only float32 operations for maximum compatibility. The converted models are then saved as `.tflite` files, ready for inference on devices that support TFLite. This approach ensures that the models maintain their original precision and are optimized for efficient execution on a wide range of hardware.\n",
    "\n",
    "### Converting the model to Float32 (CPU, GPU and NNAPI)\n",
    "The following code block iterates over both the YOLO-NAS S and YOLO-hagRID models, loading each TensorFlow SavedModel and converting it to the TFLite format using float32 precision. By restricting the conversion to float32 operations only, the resulting TFLite models are compatible with a wide range of hardware accelerators, including CPU, GPU, and NNAPI. Each converted model is saved as a `.tflite` file, making it ready for deployment and inference on supported devices. This process preserves the original model accuracy while enabling efficient execution on mobile and edge platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7db4e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [\"yolo_nas_s\", \"yolo_hagRID\"]:\n",
    "\n",
    "    # Step 1: Load the SavedModel.\n",
    "    saved_model_dir = f\"/models/{model}\"\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "\n",
    "    # Step 2: Restrict to float32 operations only (for max delegate\n",
    "    # compatibility).\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "\n",
    "    # Step 3: Convert the model.\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Step 4: Save the model.\n",
    "    output_path = f\"/models/{model}_float32.tflite\"\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "    print(f\"✅ Float32 model saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3ae16",
   "metadata": {},
   "source": [
    "### Converting the model to Float16 (CPU and GPU)\n",
    "This section demonstrates how to convert TensorFlow SavedModels to TensorFlow Lite (TFLite) format using float16 precision for deployment on mobile and edge devices. The process iterates over both the YOLO-NAS S and YOLO-hagRID models, loading each SavedModel and configuring the TFLiteConverter to:\n",
    "\n",
    "- Enable default optimizations for improved performance and reduced model size.\n",
    "- Target float16 as the precision for weights and activations, which reduces memory usage and speeds up inference on supported hardware (CPU and GPU).\n",
    "- Restrict the converter to use only built-in TFLite operations for maximum compatibility.\n",
    "\n",
    "The converted float16 models are saved as `.tflite` files, making them suitable for efficient inference on devices that support float16 execution. This approach balances model size, speed, and accuracy for deployment on a wide range of hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553266b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [\"yolo_nas_s\", \"yolo_hagRID\"]:\n",
    "\n",
    "    # Step 1: Load the SavedModel.\n",
    "    saved_model_dir = f\"/models/{model}\"\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "\n",
    "    # Step 2: Enable optimization.\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    # Step 3: Set float16 as the target precision.\n",
    "    converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "    # Step 4: Use only float ops (TFLITE_BUILTINS).\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "\n",
    "    # Step 5: Convert the model.\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Step 6: Save the converted model.\n",
    "    output_path = f\"/models/{model}_float16.tflite\"\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "    print(f\"✅ Float16 model saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46a3f8b",
   "metadata": {},
   "source": [
    "### Converting the model to Int8 (CPU and NNAPI)\n",
    "This section demonstrates how to convert TensorFlow SavedModels to fully quantized Int8 TFLite models for deployment on mobile and edge devices. The process involves:\n",
    "\n",
    "- Loading each SavedModel for the YOLO-NAS S and YOLO-hagRID models.\n",
    "- Enabling default optimizations to allow quantization.\n",
    "- Defining a representative dataset generator, which provides sample input data to calibrate the quantization process and ensure accuracy.\n",
    "- Restricting the converter to use only built-in TFLite operations, including Int8 ops, for maximum compatibility and performance.\n",
    "- Converting the model and saving the quantized Int8 TFLite file.\n",
    "\n",
    "This approach produces highly efficient models suitable for real-time inference on devices that support TFLite Int8 execution, leveraging hardware acceleration where available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [\"yolo_nas_s\", \"yolo_hagRID\"]:\n",
    "\n",
    "    # Step 1: Load the SavedModel.\n",
    "    saved_model_dir = f\"/models/{model}\"\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "\n",
    "    # Step 2: Enable optimizations.\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    # Step 3: Define representative dataset generator.\n",
    "    # Replace (1, 320, 320, 3) with your model's input shape if different.\n",
    "    def representative_data_gen():\n",
    "        for _ in range(100):\n",
    "            dummy_input = np.random.rand(1, 320, 320, 3).astype(np.float32)\n",
    "            yield [dummy_input]\n",
    "\n",
    "    converter.representative_dataset = representative_data_gen\n",
    "\n",
    "    # Step 4: Set supported operations and data types for full Int8.\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,      # Float32\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS_INT8  # Allow fallback ops\n",
    "    ]\n",
    "\n",
    "    # Step 5: Convert the model.\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Step 6: Save the optimized model to a file\n",
    "    output_path = f\"/models/{model}_int8.tflite\"\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "    print(f\"✅ Int8 model saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde1220",
   "metadata": {},
   "source": [
    "By following these steps, the model is optimized for efficient inference on Qualcomm® platforms, leveraging hardware acceleration for real-time AI applications. This process ensures that the model is both accurate and performant, making it suitable for deployment in edge devices powered by Qualcomm® chipsets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
