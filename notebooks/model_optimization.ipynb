{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdfd714b",
   "metadata": {},
   "source": [
    "# Model Optimization Notebook\n",
    "\n",
    "This Jupyter Notebook demonstrates the process of preparing image data, downloading the COCO dataset, preprocessing images, and converting a YOLO NAS ONNX model to various SNPE DLC formats for deployment. The workflow includes:\n",
    "\n",
    "- Importing necessary libraries for image processing and file management.\n",
    "- Downloading and extracting a subset of the COCO validation dataset.\n",
    "- Preprocessing images to the required input format for model inference.\n",
    "- Converting the YOLO NAS ONNX model to SNPE DLC format, including quantization and graph preparation for specific hardware targets.\n",
    "- Documenting each step for reproducibility and clarity.\n",
    "\n",
    "This notebook serves as a practical guide for deploying deep learning models on Qualcomm platforms using the SNPE toolkit.\n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. **Build and start the Docker Compose environment** as described in the project documentation.\n",
    "2. **Access this notebook** in your browser at:  \n",
    "    [http://127.0.0.1:8888/notebooks/model_optimization.ipynb](http://127.0.0.1:8888/notebooks/model_optimization.ipynb)\n",
    "3. **Run all cells** in order to optimiza the ONNX model to Qualcomm chipsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa434004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682e84b",
   "metadata": {},
   "source": [
    "## Data cleaning.\n",
    "\n",
    "The `preprocess` function resizes an input image to 320x320 pixels, normalizes its pixel values to the range [0, 1], and returns the processed image as a NumPy array of type float32, preparing it for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fafef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(original_image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess the input image for model inference.\n",
    "\n",
    "    Args:\n",
    "        original_image (np.ndarray): The input image in BGR format.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The preprocessed image in the format expected by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Resize the image to 320x320 pixels and normalize pixel values to [0, 1].\n",
    "    resized_image = cv.resize(original_image, (320, 320))\n",
    "    return (resized_image / 255.).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a94d9c",
   "metadata": {},
   "source": [
    "### Getting the coco dataset\n",
    "The COCO (Common Objects in Context) dataset is a large-scale image dataset designed for object detection, segmentation, and captioning tasks. In this pipeline, we use a subset of the COCO validation images to test and optimize our deep learning model. The images are downloaded, preprocessed, and converted into a raw format suitable for model inference and quantization steps. This ensures that the model is evaluated and optimized using real-world, diverse data representative of common objects and scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0d862-08db-4b97-8304-d2649bdb49e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"val2017.zip\"):\n",
    "    !wget http://images.cocodataset.org/zips/val2017.zip -q --show-progress\n",
    "\n",
    "if not os.path.exists(\"val2017\"):\n",
    "    !unzip val2017.zip\n",
    "\n",
    "if not os.path.exists(\"raw\"):\n",
    "    !mkdir \"raw\"\n",
    "    filenames = glob.glob(\"raw/*.jpg\")\n",
    "    if len(filenames) < 15:\n",
    "        filenames = glob.glob(\"val2017/*.jpg\")[:15]\n",
    "        for filename in filenames:\n",
    "            image = cv.imread(filename)\n",
    "            image = preprocess(image)\n",
    "            image.tofile(\n",
    "                filename.replace(\"val2017\", \"raw\").replace(\".jpg\", \".raw\")\n",
    "            )\n",
    "\n",
    "!zsh -c 'find raw -name \"*.raw\" > ./raw/input.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b3cf9",
   "metadata": {},
   "source": [
    "## Model Optimization\n",
    "This section covers the process of optimizing a deep learning model for deployment on Qualcomm® chipsets using the Qualcomm® Neural Processing SDK for AI (SNPE). The workflow includes converting a YOLO NAS ONNX model to the SNPE DLC format, quantizing the model for efficient inference, and preparing the model for specific hardware targets.\n",
    "\n",
    "### 1. Model Conversion\n",
    "\n",
    "The first step is to convert the ONNX model to the SNPE Deep Learning Container (DLC) format. This is achieved using the `snpe-onnx-to-dlc` tool, which translates the ONNX model into a format compatible with Qualcomm® hardware accelerators.\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-onnx-to-dlc -i /models/yolo_nas_s.onnx -o /models/yolo_nas_s_fp32.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1798bae-ec56-417c-b41b-7704ad8261ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-onnx-to-dlc -i /models/yolo_nas_s.onnx -o /models/yolo_nas_s_fp32.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe981f",
   "metadata": {},
   "source": [
    "### 2. Model Inspection\n",
    "\n",
    "After conversion, the `snpe-dlc-info` tool is used to inspect the DLC file. This step ensures that the model has been correctly converted and provides information about the model's input and output tensors.\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-dlc-info -i /models/yolo_nas_s_fp32.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-dlc-info -i /models/yolo_nas_s_fp32.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb9fc9",
   "metadata": {},
   "source": [
    "### 3. Model Quantization\n",
    "\n",
    "Quantization reduces the model size and increases inference speed by converting floating-point weights to 8-bit integers. The `snpe-dlc-quantize` tool uses a calibration dataset (prepared in the previous steps) to optimize the model for INT8 precision.\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-dlc-quantize --input_dlc /models/yolo_nas_s_fp32.dlc --input_list ./raw/input.txt --output_dlc /models/yolo_nas_s_int8.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abad4e88-e265-46e8-9b5e-8c87d86ffa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-dlc-quantize --input_dlc /models/yolo_nas_s_fp32.dlc --input_list ./raw/input.txt --output_dlc /models/yolo_nas_s_int8.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf28a16",
   "metadata": {},
   "source": [
    "### 4. Post-Quantization Inspection\n",
    "\n",
    "After quantization, the model is inspected again to verify the changes and ensure the quantized model is ready for deployment.\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-dlc-info -i /models/yolo_nas_s_int8.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-dlc-info -i /models/yolo_nas_s_int8.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c66503",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Hardware-Specific Graph Preparation\n",
    "\n",
    "To further optimize the model for a specific Qualcomm® SoC (e.g., SM7325), the `snpe-dlc-graph-prepare` tool is used. This step configures the model's output tensors and prepares it for execution on the target hardware's HTP (Hexagon Tensor Processor).\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-dlc-graph-prepare --input_dlc /models/yolo_nas_s_int8.dlc --set_output_tensors=output_bboxes,output_classes --htp_socs=sm7325 --output_dlc=/models/yolo_nas_s_int8_htp_sm7325.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aeeea1-9429-447a-b4a9-894c45e6bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-dlc-graph-prepare --input_dlc /models/yolo_nas_s_int8.dlc --set_output_tensors=output_bboxes,output_classes --htp_socs=sm7325 --output_dlc=/models/yolo_nas_s_int8_htp_sm7325.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5cec5",
   "metadata": {},
   "source": [
    "### 6. Final Model Inspection\n",
    "\n",
    "A final inspection confirms that the model is correctly prepared for the target hardware and ready for deployment.\n",
    "\n",
    "**Command:**\n",
    "```\n",
    "snpe-dlc-info -i /models/yolo_nas_s_int8_htp_sm7325.dlc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a22362",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zsh -c 'snpe-dlc-info -i /models/yolo_nas_s_int8_htp_sm7325.dlc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde1220",
   "metadata": {},
   "source": [
    "By following these steps, the model is optimized for efficient inference on Qualcomm® platforms, leveraging hardware acceleration for real-time AI applications. This process ensures that the model is both accurate and performant, making it suitable for deployment in edge devices powered by Qualcomm® chipsets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
