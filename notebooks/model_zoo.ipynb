{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f48ba2a1",
   "metadata": {},
   "source": [
    "# Model Zoo Notebook\n",
    "\n",
    "This notebook demonstrates how to load a COCO-pretrained YOLO-NAS S model and a hagRID-pretrained YOLO 11 model, and export them to ONNX format using PyTorch, SuperGradients, and Ultralytics.\n",
    "\n",
    "**Important:**  \n",
    "This notebook must be executed first before running any other notebooks in this project. It prepares the modela and exports them for further use.\n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. **Build and start the Docker Compose environment** as described in the project documentation.\n",
    "2. **Access this notebook** in your browser at:  \n",
    "    [http://127.0.0.1:8889/notebooks/model_zoo.ipynb](http://127.0.0.1:8889/notebooks/model_zoo.ipynb)\n",
    "3. **Run all cells** in order to prepare the models for downstream tasks.\n",
    "\n",
    "Make sure to follow these steps to ensure the environment and model are set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf30e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-31 09:22:36] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The console stream is logged into /root/sg_logs/console.log\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries.\n",
    "from typing import List\n",
    "\n",
    "import onnx\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from onnx import helper, TensorProto\n",
    "from super_gradients.common.object_names import Models\n",
    "from super_gradients.training import models\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189e0db",
   "metadata": {},
   "source": [
    "## Exporting YOLO-NAS S Model to ONNX\n",
    "\n",
    "The following code cell loads a COCO-pretrained YOLO-NAS S model using SuperGradients, prepares it for ONNX export, and saves the exported model to the `./models/yolo_nas_s.onnx` path. The process includes:\n",
    "\n",
    "- Loading the pretrained model and setting it to evaluation mode.\n",
    "- Preparing the model for conversion with a specified input size.\n",
    "- Creating a dummy input tensor to simulate a real input.\n",
    "- Defining input and output names for the ONNX graph.\n",
    "- Exporting the model to ONNX format using `torch.onnx.export`.\n",
    "\n",
    "This ONNX model can then be used for inference in other frameworks or deployment environments that support ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99683984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-31 09:22:38] WARNING - checkpoint_utils.py - :warning: The pre-trained models provided by SuperGradients may have their own licenses or terms and conditions derived from the dataset used for pre-training.\n",
      " It is your responsibility to determine whether you have permission to use the models for your use case.\n",
      " The model you have requested was pre-trained on the coco dataset, published under the following terms: https://cocodataset.org/#termsofuse\n",
      "[2025-05-31 09:22:38] INFO - checkpoint_utils.py - License Notification: YOLO-NAS pre-trained weights are subjected to the specific license terms and conditions detailed in \n",
      "https://github.com/Deci-AI/super-gradients/blob/master/LICENSE.YOLONAS.md\n",
      "By downloading the pre-trained weight files you agree to comply with these terms.\n",
      "[2025-05-31 09:22:38] INFO - checkpoint_utils.py - Successfully loaded pretrained weights for architecture yolo_nas_s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is valid!\n"
     ]
    }
   ],
   "source": [
    "# Load a COCO-pretrained YOLO-NAS S model.\n",
    "model = models.get(Models.YOLO_NAS_S, pretrained_weights=\"coco\")\n",
    "model.eval()\n",
    "\n",
    "# Prepare the model for ONNX conversion.\n",
    "model.prep_model_for_conversion(input_size=[1, 3, 320, 320])\n",
    "\n",
    "# Define a dummy input tensor with the expected shape.\n",
    "dummy_input = torch.randn([1, 3, 320, 320], device=\"cpu\")\n",
    "\n",
    "# Specify the input and output names for the ONNX model.\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output_bboxes\", \"output_classes\"]\n",
    "\n",
    "# Export the model to ONNX format.\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"/models/yolo_nas_s.onnx\",\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    opset_version=11\n",
    ")\n",
    "\n",
    "# Validate the ONNX model.\n",
    "model = onnx.load(\"/models/yolo_nas_s.onnx\")\n",
    "onnx.checker.check_model(model)\n",
    "print(\"Model is valid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc31aed",
   "metadata": {},
   "source": [
    "## ONNX Model Graph Transformation Utility\n",
    "\n",
    "The following code cell defines a utility function, `transform_io_and_prune_node`, which modifies an ONNX model's computation graph. This function enables advanced manipulation of ONNX models, including:\n",
    "\n",
    "- Renaming the model's input tensor.\n",
    "- Removing all existing outputs and creating new output chains using Transpose and Identity nodes.\n",
    "- Assigning new names to the outputs and connecting them to specified internal tensors.\n",
    "- Removing a specific node (e.g., a Concat node) from the model by its name.\n",
    "\n",
    "This utility is useful for customizing ONNX models for downstream tasks, such as adapting the model's input/output interface or pruning unnecessary nodes before deployment. The function takes paths to the input and output ONNX models, new input/output names, internal tensor names for output chaining, and the name of the node to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb3afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_io_and_prune_node(\n",
    "    model_path: str, output_path: str,\n",
    "    new_input_name: str,\n",
    "    new_output_names: List,\n",
    "    internal_tensor_names: List,\n",
    "    remove_node_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Transforms the input and output of an ONNX model, renaming the input,\n",
    "    removing all outputs, and creating a new output chain with Transpose and\n",
    "    Identity nodes. Also removes a specified node from the model.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the input ONNX model.\n",
    "        output_path (str): Path to save the modified ONNX model.\n",
    "        new_input_name (str): New name for the input tensor.\n",
    "        new_output_names (List[str]): List of new names for the output tensors.\n",
    "        internal_tensor_names (List[str]): List of internal tensor names to be\n",
    "            used in the new output chain.\n",
    "        remove_node_name (str): Name of the node to be removed from the model.\n",
    "    \"\"\"\n",
    "    model = onnx.load(model_path)\n",
    "\n",
    "    # Step 1: Rename input.\n",
    "    old_input_name = model.graph.input[0].name\n",
    "    model.graph.input[0].name = new_input_name\n",
    "    for node in model.graph.node:\n",
    "        node.input[:] = [\n",
    "            new_input_name if i == old_input_name else i for i in node.input\n",
    "        ]\n",
    "\n",
    "    # Step 2: Remove all outputs.\n",
    "    del model.graph.output[:]\n",
    "\n",
    "    # Step 3: Build new output chain: Transpose â†’ Identity â†’ output.\n",
    "    transpose_axes = [\n",
    "        [0, 2, 1],  # Same for both outputs.\n",
    "        [0, 2, 1],  # Same for both outputs.\n",
    "    ]\n",
    "    output_shapes = [\n",
    "        [1, 8400,  4],  # For output_bboxes, for example\n",
    "        [1, 8400, 34],  # For output_classes, for example\n",
    "    ]\n",
    "\n",
    "    new_nodes = []\n",
    "    for i, (new_name, internal_name) in enumerate(\n",
    "        zip(new_output_names, internal_tensor_names)\n",
    "    ):\n",
    "        transpose_name = f\"{new_name}_transposed\"\n",
    "\n",
    "        # Transpose node.\n",
    "        transpose_node = helper.make_node(\n",
    "            \"Transpose\",\n",
    "            inputs=[internal_name],\n",
    "            outputs=[transpose_name],\n",
    "            perm=transpose_axes[i],\n",
    "            name=f\"Transpose_{new_name}\"\n",
    "        )\n",
    "\n",
    "        # Identity node to assign output name.\n",
    "        identity_node = helper.make_node(\n",
    "            \"Identity\",\n",
    "            inputs=[transpose_name],\n",
    "            outputs=[new_name],\n",
    "            name=f\"Identity_{new_name}\"\n",
    "        )\n",
    "\n",
    "        # Output metadata.\n",
    "        output_info = helper.make_tensor_value_info(\n",
    "            new_name,\n",
    "            TensorProto.FLOAT,\n",
    "            output_shapes[i]\n",
    "        )\n",
    "\n",
    "        # Append to graph.\n",
    "        new_nodes.extend([transpose_node, identity_node])\n",
    "        model.graph.output.append(output_info)\n",
    "\n",
    "    # Step 4: Remove the Concat node by name.\n",
    "    original_nodes = [\n",
    "        node for node in model.graph.node if node.name != remove_node_name\n",
    "    ]\n",
    "\n",
    "    # Step 5: Replace node list with cleaned + new output nodes.\n",
    "    model.graph.ClearField(\"node\")\n",
    "    model.graph.node.extend(original_nodes + new_nodes)\n",
    "\n",
    "    # Save model.\n",
    "    onnx.save(model, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07507b68",
   "metadata": {},
   "source": [
    "## Downloading the hagRID YOLO 11 Model Weights\n",
    "\n",
    "The following code cell downloads the pretrained YOLO 11 model weights from Hugging Face and saves them to the `/models` directory. These weights are required for loading the hagRID-pretrained YOLO model and exporting it to ONNX format in subsequent steps. Make sure the download completes successfully before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f768729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/models/best.pt     100%[===================>]  15.34M  13.1MB/s    in 1.2s    \n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/testdummyvt/hagRIDv2_gesture_det_models/resolve/main/yolo11n_10GB/train/weights/best.pt -q --show-progress -O /models/best.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a946685",
   "metadata": {},
   "source": [
    "## Exporting and Transforming the hagRID YOLO 11 Model\n",
    "\n",
    "The following code cell performs several key steps to prepare the hagRID-pretrained YOLO 11 model for downstream use:\n",
    "\n",
    "- **Load the YOLO 11 model** using the downloaded weights.\n",
    "- **Export the model to ONNX format** with the appropriate opset version.\n",
    "- **Load the exported ONNX model** for further processing.\n",
    "- **Transform the ONNX model** by:\n",
    "    - Renaming the input and outputs to standardized names.\n",
    "    - Creating new output chains for bounding boxes and class predictions.\n",
    "    - Removing an unnecessary node from the computation graph.\n",
    "- **Clean up** by deleting the original `.pt` and intermediate ONNX files to conserve disk space.\n",
    "\n",
    "This process ensures the exported ONNX model is optimized and ready for integration into other applications or inference pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "194f23e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.146 ðŸš€ Python-3.10.16 torch-2.7.0+cu126 CPU (13th Gen Intel Core(TM) i9-13950HX)\n",
      "YOLO11n summary (fused): 100 layers, 2,588,782 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/models/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 38, 8400) (15.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 11...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.54...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 0.6s, saved as '/models/best.onnx' (10.1 MB)\n",
      "\n",
      "Export complete (0.8s)\n",
      "Results saved to \u001b[1m/models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/models/best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/models/best.onnx imgsz=640 data=/mnt/batch/tasks/shared/LS_root/mounts/clusters/t4-two/data/hagRIDv2_512px_10GB/yolo_format/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Model is valid!\n"
     ]
    }
   ],
   "source": [
    "# Load the model.\n",
    "model = YOLO(\"/models/best.pt\")\n",
    "\n",
    "# Export to ONNX.\n",
    "model.export(format=\"onnx\", opset=11)\n",
    "\n",
    "# Transform the model by renaming inputs, outputs, and pruning a node.\n",
    "transform_io_and_prune_node(\n",
    "    model_path=\"/models/best.onnx\",\n",
    "    output_path=\"/models/yolo_hagRID.onnx\",\n",
    "    new_input_name=\"input\",\n",
    "    new_output_names=[\n",
    "        \"output_bboxes\",\n",
    "        \"output_classes\"\n",
    "    ],\n",
    "    internal_tensor_names=[\n",
    "        \"/model.23/Mul_2_output_0\",\n",
    "        \"/model.23/Sigmoid_output_0\"\n",
    "    ],\n",
    "    remove_node_name=\"/model.23/Concat_5\"\n",
    ")\n",
    "\n",
    "# Validate the ONNX model.\n",
    "model = onnx.load(\"/models/yolo_hagRID.onnx\")\n",
    "onnx.checker.check_model(model)\n",
    "print(\"Model is valid!\")\n",
    "\n",
    "# Delete the original .pt and ONNX models to save space.\n",
    "os.remove(\"/models/best.pt\")\n",
    "os.remove(\"/models/best.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e399f5",
   "metadata": {},
   "source": [
    "## Notebook Complete\n",
    "\n",
    "You have now successfully prepared and exported both the COCO-pretrained YOLO-NAS S model and the hagRID-pretrained YOLO 11 model to ONNX format. These models are now ready for use in downstream tasks, such as inference, benchmarking, or integration into deployment pipelines.\n",
    "\n",
    "If you encounter any issues or need to re-run the setup, simply restart the notebook and execute all cells in order.\n",
    "\n",
    "For further information, refer to the project documentation or explore the exported ONNX models in the `/models` directory.\n",
    "\n",
    "---\n",
    "**Next Steps:**\n",
    "- Use the exported ONNX models in your inference or deployment workflows.\n",
    "- Explore other notebooks in this project for additional model processing and evaluation tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
